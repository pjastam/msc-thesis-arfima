# Modeling economic and non-economic data {#modeling}

In this section we give an empirical illustration of the theory presented in the sections before. In the section 4.1 we describe the maximum likelihood computer codes that we applied for estimation of nonfractional and fractional ARMA models. In both codes the input data is standardized initially: its estimated mean is subtracted and the result is divided by its estimated standard deviation. The value of the likelihood function is corrected for this standardization afterwards. In section 4.2 the long--- and short---memory behavior of quarterly real US gross national product is investigated. In section 4.3 we determine the long--- and short memory characteristics of the annual Trier oak tree ring widths series. The length of this series is large as compared to most economic series and is therefore interesting for long-memory studies.

## Annual Trier oak tree ring widths

Load data

```{r}
library(readr)
OAKV21 <- read_csv("C:/DOS/scriptie_diskette2/DATA/OAKV21.DAT",col_names = c("y"))
y <- OAKV21$y
t <- seq(822,1964,1)
dy <- diff(y, differences = 1)
dt <- seq(823,1964,1)
```

Data exploration

Figure 4.14 (p. 95)

```{r}
library("TSA")
par(mfrow = c(4, 2))
plot(t,y,type='l',main='Trier Oak Tree Ring Widths',xlab="time",ylab="");
legend("topright",legend=c(bquote(mean: .(round(mean(y),4))),bquote(variance: .(round(var(y),4)))), bty = "n")
acf(y,lag.max=20,type="correlation",main="Autocorrelation Coefficients",xlab="lags 1-20",ylab="")
periodogram(y,main="Periodogram",xlab=expression("Fractions of" ~ 2*pi),ylab="");  abline(h=0)
acf(y,lag.max=20,type="partial",main="Partial Autocorrelation Coefficients",xlab="lags 1-20",ylab="")

plot(dt,dy,type='l',main='First Differences',xlab="time",ylab="");
legend("topright",legend=c(bquote(mean: .(round(mean(dy),4))),bquote(variance: .(round(var(dy),4)))), bty = "n")
acf(dy,lag.max=20,type="correlation",main="Autocorrelation Coefficients",xlab="lags 1-20",ylab="")
periodogram(dy,main="Periodogram",xlab=expression("Fractions of" ~ 2*pi),ylab="");  abline(h=0)
acf(dy,lag.max=20,type="partial",main="Partial Autocorrelation Coefficients",xlab="lags 1-20",ylab="")
par(mfrow = c(1, 1))
```

### Augmented Dickey-Fuller tests (p. 96)

We perform an ADF test of the levels and first differences and observe in case of the latter that the null hypothesis of stationarity can not be rejected for any of the lags up to 50.

```{r}
library("aTSA")
adf_y <- adf.test(y, nlag = 51, output = TRUE)$type3
plot(adf_y[,1],adf_y[,2],main="ADF t statistic",xlab="lags",ylab="",type="l",ylim=c(min(adf_y[,2]),0)); abline(h=-3.41, col = "gray60")
adf_dy <- adf.test(dy, nlag = 51, output = TRUE)$type3
plot(adf_dy[,1],adf_dy[,2],main="ADF t statistic",xlab="lags",ylab="",type="l",ylim=c(min(adf_y[,2]),0)); abline(h=-3.41, col = "gray60")
```

Standardize data

```{r}
#y <- (y-mean(y))/sd(y)
#dy <- diff(y, differences = 1)
```

### Model estimation - ARIMA models estimated by ML (p. 96)

```{r}
fit_dy <- fit_arima(dy,2,0,1)
est_stats(fit_dy,logLik(fit_dy),length(fit_dy$coef))
```

The performance measures 2lnL, AIC and SIC are identical to those reported for the ARMA(2,1) model in Table 4.7 of my MSc thesis at p. 98.

### Model estimation - ARFIMA models estimated by ML, levels (p. 100)

Package used: (arfima::arfima)

We try two different packages to estimate the ARFIMA models. The first is the package arfima.

```{r}
fit_y <- fit_arfima1(y,2,0,0)
```

```{r}
est_stats(fit_y,fit_y$modes[[2]]$loglik,length(fit_y$modes[[2]]$pars))
```

Note that two sets of coefficients result if we try to estimate an ARFIMA(2,1,0) model for the levels y. The second set of coefficients resembles that in the MSc thesis on p. 101. However, the AIC and SIC deviate a lot.

### Model estimation - ARFIMA models estimated by ML, first differences (p. 102)

If we apply this estimation method on the first differences, then the results are as follows.

```{r}
fit_dy <- fit_arfima1(y,3,1,2)
```

```{r}
est_stats(fit_dy$modes[[1]],fit_dy$modes[[1]]$loglik,length(fit_dy$modes[[1]]$pars))
```

### Alternative model estimation - ARFIMA models estimated by ML, levels (p. 100)

Package used: (fracdiff::arfima)

Fracdiff is the second package that we try.

```{r}
fit_y <- fit_arfima2(y,2,0)
```

De schattingen wijken ietwat af van die in de MSc worden genoemd (p. 101).

```{r}
est_stats(fit_y,logLik(fit_y),nrow(fit_y$covariance.dpq))
```

Now, the coefficients slightly deviate from those in the MSc thesis, but the AIC and SIC are much closer (p. 102).

### Alternative model estimation - ARFIMA models estimated by ML, first differences (p. 102)

```{r}
fit_dy <- fit_arfima2(dy,3,2)
```

### Automatic choice of AR- and MA-orders

If we want to let the computer pick the optimal model automatically, we can use the package forecast.

```{r}
library("forecast")
fit <- arfima(y, estim = "mle")
summary(fit)
#tsdisplay(residuals(fit))

p <- length(fit$ar)
intindx <- length(fit$d)
q <- length(fit$ar)
npar=intindx+p+q+1
paste("2lnL             = ",2*logLik(fit)) #logLik(fit)
paste("2lnL - 2k/T      = ",2*logLik(fit)-2*(npar-1)) #AIC(fit) (npar+1)
paste("2lnL - lnT * k/T = ",2*logLik(fit)-(npar-1)*log(length(y))) #BIC(fit) (npar+1)
detach(package:forecast)
```
